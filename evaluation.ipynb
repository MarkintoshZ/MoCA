{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3921c0-2ce7-4120-b7a8-21e30e31f1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758b9c14-c8b7-4fe5-bc61-11badf241add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoTokenizer, pipeline\n",
    "import evaluate\n",
    "from evaluate import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdfc0028-99e7-4680-b5bc-69df74c47c21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/markintosh/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "Loading cached processed dataset at /home/markintosh/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-e6a8566197def2bc.arrow\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "cnn_test = load_dataset(\"cnn_dailymail\", '3.0.0', split=\"test\")\n",
    "\n",
    "checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"article\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"highlights\"], max_length=142, \n",
    "                       truncation=True, padding=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_test = cnn_test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a62a038d-bc83-40e1-a739-cfcd9f41465e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluation setup\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a0517c5-b4b5-4071-8b48-d5fded565503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 8.2366304397583,\n",
       " 'eval_rouge1': 0.3439,\n",
       " 'eval_rouge2': 0.1441,\n",
       " 'eval_rougeL': 0.2463,\n",
       " 'eval_rougeLsum': 0.2855,\n",
       " 'eval_gen_len': 66.694,\n",
       " 'eval_runtime': 364.4356,\n",
       " 'eval_samples_per_second': 2.744,\n",
       " 'eval_steps_per_second': 0.274}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model_ckpt = \"facebook/bart-large-cnn\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"BART\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(tokenized_test.select(range(1000)))\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9187ba4-b8a5-4b0e-b8bb-396074b01f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 7.847792148590088,\n",
       " 'eval_rouge1': 0.3722,\n",
       " 'eval_rouge2': 0.1564,\n",
       " 'eval_rougeL': 0.2622,\n",
       " 'eval_rougeLsum': 0.3047,\n",
       " 'eval_gen_len': 68.436,\n",
       " 'eval_runtime': 403.6445,\n",
       " 'eval_samples_per_second': 2.477,\n",
       " 'eval_steps_per_second': 0.248}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model_ckpt = './checkpoints2023-05-06T13:00:32.263295'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"BART\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(tokenized_test.select(range(1000)))\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e631499a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 8.471336364746094,\n",
       " 'eval_rouge1': 0.3672,\n",
       " 'eval_rouge2': 0.1566,\n",
       " 'eval_rougeL': 0.2573,\n",
       " 'eval_rougeLsum': 0.2988,\n",
       " 'eval_gen_len': 69.594,\n",
       " 'eval_runtime': 316.1945,\n",
       " 'eval_samples_per_second': 3.163,\n",
       " 'eval_steps_per_second': 0.316}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model_ckpt = './checkpoints2023-05-06T16:43:22.416287/'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"BART\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(tokenized_test.select(range(1000)))\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89a2ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='785' max='1149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 785/1149 45:25 < 21:05, 0.29 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.863325595855713,\n",
       " 'eval_rouge1': 0.4618,\n",
       " 'eval_rouge2': 0.2235,\n",
       " 'eval_rougeL': 0.32,\n",
       " 'eval_rougeLsum': 0.3912,\n",
       " 'eval_gen_len': 74.1923,\n",
       " 'eval_runtime': 4150.6028,\n",
       " 'eval_samples_per_second': 2.768,\n",
       " 'eval_steps_per_second': 0.277}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model_ckpt = './checkpoints2023-05-06T13:00:32.263295'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"BART\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(tokenized_test)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15bbf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='266' max='1149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 266/1149 14:58 < 49:53, 0.29 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 7.3013129234313965,\n",
       " 'eval_rouge1': 0.4576,\n",
       " 'eval_rouge2': 0.2227,\n",
       " 'eval_rougeL': 0.3167,\n",
       " 'eval_rougeLsum': 0.3871,\n",
       " 'eval_gen_len': 77.4565,\n",
       " 'eval_runtime': 4409.3738,\n",
       " 'eval_samples_per_second': 2.606,\n",
       " 'eval_steps_per_second': 0.261}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model_ckpt = './checkpoints2023-05-12T20:18:24.824529/'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"BART\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(tokenized_test)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0218f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='272' max='1149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 272/1149 17:03 < 55:12, 0.26 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 7.935605525970459,\n",
       " 'eval_rouge1': 0.4604,\n",
       " 'eval_rouge2': 0.2261,\n",
       " 'eval_rougeL': 0.3164,\n",
       " 'eval_rougeLsum': 0.4293,\n",
       " 'eval_gen_len': 87.524,\n",
       " 'eval_runtime': 4951.5594,\n",
       " 'eval_samples_per_second': 2.32,\n",
       " 'eval_steps_per_second': 0.232}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model_ckpt = './checkpoints2023-05-14T18:15:05.361400/'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"BART\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(tokenized_test)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df919bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='758' max='1149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 758/1149 48:02 < 24:48, 0.26 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 8.115837097167969,\n",
       " 'eval_rouge1': 0.4597,\n",
       " 'eval_rouge2': 0.2255,\n",
       " 'eval_rougeL': 0.3175,\n",
       " 'eval_rougeLsum': 0.4295,\n",
       " 'eval_gen_len': 86.2607,\n",
       " 'eval_runtime': 4619.5407,\n",
       " 'eval_samples_per_second': 2.487,\n",
       " 'eval_steps_per_second': 0.249}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model_ckpt = './checkpoints2023-05-14T19:42:34.263202/'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"BART\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(tokenized_test)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f242d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
